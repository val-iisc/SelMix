# config.yaml

# Saving & loading of the model
save_dir: '.'
save_name: junk
resume: true
load_path: './saved_models/CIFAR10_N1-1500_M1-3000_IBRL-100_IBRU-100_LA_seed-0/model_best.pth'
overwrite: true
nestrov: false
freeze_backbone: false

# Training Configuration of FixMatch
M: 'g_mean'
epoch: 1
num_train_iter: 10000
num_eval_iter: 100
num_labels: 15000
batch_size: 64
eval_batch_size: 1024
DistTemp: 0.1
percentile: 100.0
ema_m: 0.999
ema_v: 1.0
mixup_lambda_min: 0.6
filter: false
conf_thresh: null
uratio: 2

# Optimizer configurations
lr: 0.0003
min_lr: 0.00
val_lr: 0.1
momentum: 0.9
damp: 0.9
weight_decay: 0.0005
amp: false
p_ema: 1.0
T: 1.0
opt: 'SGD'

# Backbone Net Configurations
net: 'wide_resnet28_2'
update_type: 'EG'
net_from_name: false
depth: 28
widen_factor: 2
leaky_slope: 0.1
dropout: 0.0
bn_momentum: 0.00
min_gain: 0.0
beta: 0.0
alpha: 0.95
tau: 0.1
lambda_max: 100.0

# Data Configurations
data_dir: './data'
dataset: 'cifar10'
train_sampler: 'RandomSampler'
num_classes: 10
num_workers: 1
imbalance_l: 100.0
frac: 1.0
imbalance_u: 100.0
size: 32
N1: 1500
M1: 3000
lt: false
train_backbone: true
mask: false
cutmix: false
puzzlemix: false
vmix: false

# multi-GPUs & Distributed Training
world_size: 1
rank: 0
dist_url: 'tcp://127.0.0.1:10001'
dist_backend: 'nccl'
seed: 0
gpu: null
multiprocessing_distributed: false

# wandb logging
wandb_project: 'ICML-CIFAR10-IBRLU-100-100'
wandb_entity: 'aaai-23'
wandb_runid: 'gm_recall_verify_seed-0'